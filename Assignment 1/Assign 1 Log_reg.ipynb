{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.multiprocessing\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from torch.utils.data import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import torch.nn.functional as fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "435f749de8534afb954be27fa7e02ef6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26421880 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "166ca2bb33094cab94aed514f6055cef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29515 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dca7d645d01c4b36893717f3de05071a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4422102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "020b7b47db25469096f229c1ab3330a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset FashionMNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: data\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset FashionMNIST\n",
       "    Number of datapoints: 10000\n",
       "    Root location: data\n",
       "    Split: Test\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "train_loader = DataLoader(training_data,batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticReg(torch.nn.Module):\n",
    "\n",
    "    def __init__(self,n_input,n_output):\n",
    "        super(LogisticReg,self).__init__()\n",
    "        self.linear = torch.nn.Linear(n_input,n_output)\n",
    "\n",
    "    def forward(self,x):\n",
    "        y_pred = torch.sigmoid(self.linear(x))\n",
    "        return y_pred\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input = 28*28\n",
    "n_output = 10\n",
    "log_reg = LogisticReg(n_input,n_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(log_reg.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.129301  [  100/  600]\n",
      "loss: 2.122943  [10100/  600]\n",
      "loss: 2.116212  [20100/  600]\n",
      "loss: 2.094087  [30100/  600]\n",
      "loss: 2.091829  [40100/  600]\n",
      "loss: 2.079532  [50100/  600]\n",
      "Epoch: 0. Loss: 2.0806353092193604. Accuracy: 62.67\n",
      "loss: 2.089301  [  100/  600]\n",
      "loss: 2.083323  [10100/  600]\n",
      "loss: 2.080890  [20100/  600]\n",
      "loss: 2.056229  [30100/  600]\n",
      "loss: 2.056772  [40100/  600]\n",
      "loss: 2.047020  [50100/  600]\n",
      "Epoch: 1. Loss: 2.0494801998138428. Accuracy: 63.59\n",
      "loss: 2.056729  [  100/  600]\n",
      "loss: 2.050426  [10100/  600]\n",
      "loss: 2.051543  [20100/  600]\n",
      "loss: 2.025636  [30100/  600]\n",
      "loss: 2.027632  [40100/  600]\n",
      "loss: 2.020263  [50100/  600]\n",
      "Epoch: 2. Loss: 2.0235908031463623. Accuracy: 64.16\n",
      "loss: 2.029698  [  100/  600]\n",
      "loss: 2.022805  [10100/  600]\n",
      "loss: 2.026738  [20100/  600]\n",
      "loss: 2.000527  [30100/  600]\n",
      "loss: 2.003052  [40100/  600]\n",
      "loss: 1.997753  [50100/  600]\n",
      "Epoch: 3. Loss: 2.0016634464263916. Accuracy: 64.41\n",
      "loss: 2.006927  [  100/  600]\n",
      "loss: 1.999371  [10100/  600]\n",
      "loss: 2.005496  [20100/  600]\n",
      "loss: 1.979600  [30100/  600]\n",
      "loss: 1.982055  [40100/  600]\n",
      "loss: 1.978491  [50100/  600]\n",
      "Epoch: 4. Loss: 1.9828130006790161. Accuracy: 64.61\n",
      "loss: 1.987473  [  100/  600]\n",
      "loss: 1.979270  [10100/  600]\n",
      "loss: 1.987100  [20100/  600]\n",
      "loss: 1.961897  [30100/  600]\n",
      "loss: 1.963907  [40100/  600]\n",
      "loss: 1.961774  [50100/  600]\n",
      "Epoch: 5. Loss: 1.9664061069488525. Accuracy: 64.65\n",
      "loss: 1.970634  [  100/  600]\n",
      "loss: 1.961830  [10100/  600]\n",
      "loss: 1.971013  [20100/  600]\n",
      "loss: 1.946712  [30100/  600]\n",
      "loss: 1.948051  [40100/  600]\n",
      "loss: 1.947096  [50100/  600]\n",
      "Epoch: 6. Loss: 1.951972246170044. Accuracy: 64.92\n",
      "loss: 1.955881  [  100/  600]\n",
      "loss: 1.946534  [10100/  600]\n",
      "loss: 1.956821  [20100/  600]\n",
      "loss: 1.933521  [30100/  600]\n",
      "loss: 1.934057  [40100/  600]\n",
      "loss: 1.934076  [50100/  600]\n",
      "Epoch: 7. Loss: 1.9391554594039917. Accuracy: 65.11\n",
      "loss: 1.942816  [  100/  600]\n",
      "loss: 1.932985  [10100/  600]\n",
      "loss: 1.944199  [20100/  600]\n",
      "loss: 1.921929  [30100/  600]\n",
      "loss: 1.921594  [40100/  600]\n",
      "loss: 1.922426  [50100/  600]\n",
      "Epoch: 8. Loss: 1.9276797771453857. Accuracy: 65.3\n",
      "loss: 1.931137  [  100/  600]\n",
      "loss: 1.920875  [10100/  600]\n",
      "loss: 1.932892  [20100/  600]\n",
      "loss: 1.911636  [30100/  600]\n",
      "loss: 1.910404  [40100/  600]\n",
      "loss: 1.911922  [50100/  600]\n",
      "Epoch: 9. Loss: 1.9173293113708496. Accuracy: 65.44\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "Loss = []\n",
    "acc = []\n",
    "for epoch in range(epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = log_reg(images.view(-1, 28*28))\n",
    "        loss = criterion(outputs, labels)\n",
    "        # Loss.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % 100 == 0:\n",
    "            loss, current = loss.item(), (i + 1) * len(images)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{len(train_loader):>5d}]\")\n",
    "    Loss.append(loss.item())\n",
    "    correct = 0\n",
    "    for images, labels in test_loader:\n",
    "        outputs = log_reg(images.view(-1, 28*28))\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct += (predicted == labels).sum()\n",
    "    accuracy = 100 * (correct.item()) / len(test_data)\n",
    "    acc.append(accuracy)\n",
    "    print('Epoch: {}. Loss: {}. Accuracy: {}'.format(epoch, loss.item(), accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : 1.9173293113708496\n"
     ]
    }
   ],
   "source": [
    "print(f\"loss : {Loss[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 65.44\n"
     ]
    }
   ],
   "source": [
    "print(f\"accuracy : {acc[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
